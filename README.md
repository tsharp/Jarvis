[README.md](https://github.com/user-attachments/files/23788527/README.md)

# ğŸ“˜ Ollama-Pipeline-Bridge
*A universal bridge for multi-agent pipelines, chat interfaces, and memory systems.*

Ollama-Pipeline-Bridge is a modular, extensible proxy server that connects Ollama models to any chat interface, multi-agent workflow, or custom AI pipeline.  
It acts as a central integration layer between:

- chat UIs and frontends  
- specialized agents  
- a persistent memory system  
- validation/classification modules  
- the Ollama LLM backend  

With this bridge, you can build your own multi-agent architecture including tool routing, memory recall, and standardized request/response handling.

---

# ğŸš€ Features

- **âš¡ Universal integration**  
  Connects Ollama to any chat UI or external system.

- **ğŸ§© Multi-agent pipeline**  
  Build your own agents (planner, classifier, validator, persona, tool agents, etc.).

- **ğŸ§  Integrated persistent memory**  
  SQL-backed memory storage for user history, agent states, and long-term context.

- **ğŸ”Œ Modular adapter architecture**  
  Easily plug in new interfaces, tools, or external services.

- **ğŸ³ Docker-ready**  
  Full Docker setup for local or server deployments.

- **ğŸ›  Highly extensible**  
  Add new modules, agents, validators, memory types, or adapter layers.

---

# ğŸ— Project Structure

```
Ollama-Pipeline-Bridge/
â”‚
â”œâ”€ adapters/         â†’ Connectors for chat UIs and external systems
â”œâ”€ classifier/       â†’ Text classification logic and routing helpers
â”œâ”€ core/             â†’ Main pipeline + routing system
â”œâ”€ memory/           â†’ Memory interfaces and retrieval logic
â”œâ”€ modules/          â†’ Agents, tools, role modules
â”œâ”€ ollama/           â†’ Ollama API integration
â”œâ”€ sql-memory/       â†’ SQL-backed persistent memory engine
â”œâ”€ utils/            â†’ Helpers, logging, formatting, etc.
â”œâ”€ validator-service/â†’ Optional output validation service
â”‚
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ requirements.txt
â””â”€ main.py
```

---

# ğŸ§  Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Chat UI Layer                            â”‚
â”‚                  (LobeChat / OpenWebUI)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Adapter Layer                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚  LobeChat        â”‚              â”‚  OpenWebUI       â”‚        â”‚
â”‚  â”‚  Adapter         â”‚              â”‚  Adapter         â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚ Transform Request/Response â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Core Bridge Layer                          â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Layer 1: Thinking (DeepSeek-R1:8b)                   â”‚    â”‚
â”‚  â”‚  â€¢ Intent-Analyse                                      â”‚    â”‚
â”‚  â”‚  â€¢ Hallucination-Risk-Assessment                       â”‚    â”‚
â”‚  â”‚  â€¢ Memory-Need-Detection                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Memory Retrieval (Optional)                           â”‚    â”‚
â”‚  â”‚  â€¢ Facts (SQL)                                         â”‚    â”‚
â”‚  â”‚  â€¢ Embeddings (Vector Search)                          â”‚    â”‚
â”‚  â”‚  â€¢ Knowledge Graph                                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Layer 2: Control (Qwen3:4b)                          â”‚    â”‚
â”‚  â”‚  â€¢ Fact-Checking                                       â”‚    â”‚
â”‚  â”‚  â€¢ Hallucination-Detection                             â”‚    â”‚
â”‚  â”‚  â€¢ Correction-Generation                               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Layer 3: Output (Llama3.1:8b)                        â”‚    â”‚
â”‚  â”‚  â€¢ Final-Response-Generation                           â”‚    â”‚
â”‚  â”‚  â€¢ Persona-Application                                 â”‚    â”‚
â”‚  â”‚  â€¢ Streaming-Support                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  Memory Save (Optional)                                â”‚    â”‚
â”‚  â”‚  â€¢ Extract & Save Facts                                â”‚    â”‚
â”‚  â”‚  â€¢ Update Knowledge Graph                              â”‚    â”‚
â”‚  â”‚  â€¢ Generate Embeddings                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                   â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SQL Memory   â”‚  â”‚  Validator   â”‚  â”‚   MCP Hub    â”‚
â”‚   Service    â”‚  â”‚   Service    â”‚  â”‚   (Tools)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```


---


### Backend

| Komponente | Technologie | Version | Verwendung |
|------------|-------------|---------|------------|
| **Framework** | FastAPI | Latest | REST API & Async-Support |
| **Server** | Uvicorn | Latest | ASGI Server |
| **Database** | SQLite3 | 3.x | Facts, Embeddings, Graph |
| **HTTP Client** | Requests | 2.31+ | Sync HTTP (âš ï¸ problem!) |
| **Async HTTP** | httpx | 0.25+ | Partially used |
| **YAML** | PyYAML | 6.0+ | Configs & Personas |
| **MCP** | FastMCP | Latest | MCP Protocol |

### AI/ML Models (Ollama)

| Layer | Model | Size | Purpose |
|-------|-------|-------|-------|
| Thinking | DeepSeek-R1 | 8B | Reasoning & Planning |
| Control | Qwen3 | 4B | Fact-Checking |
| Output | Llama3.1 | 8B | Response Generation |
| Embeddings | mxbai-embed-large-v1 | f16 | Semantic Search |

### Container-Infrastruktur

- **Docker** - Containerization
- **Docker Compose** - Multi-service orchestration
- **Networks**: Isolated bridge networks per service

___


# ğŸ“¦ Installation

## ğŸ”§ Local Installation

```bash
git clone https://github.com/danny094/ai-proxybridge.git
cd ai-proxybridge

pip install -r requirements.txt

python main.py
```

---

## ğŸ³ Docker Deployment (Recommended)

```bash
docker compose up --build
```

The server will be available at:

```
http://localhost:8080
```

---

# âš™ï¸ Configuration

Configuration is located in:

```
config.py
```

Configurable options include:

- Ollama endpoint  
- default models  
- memory backend  
- logging  
- agent pipeline  
- adapter settings  

---

# ğŸ§ª Basic Processing Flow

1. The user sends a message to a chat frontend.  
2. An adapter converts it to the internal pipeline format.  
3. The core router selects the appropriate agent.  
4. Memory is queried for relevant context.  
5. The agent processes the request or calls Ollama.  
6. The formatted response is returned to the UI.

---

# ğŸ§± Agents / Modules

Inside `modules/` you can define unlimited custom agents:

- planner agents  
- persona agents  
- validators  
- classifier and router agents  
- tool-specific agents  

Agents can be chained or routed dynamically.

---

# ğŸ§  Memory System

The SQL-backed memory system stores:

- conversation history  
- long-term user data  
- agent state  
- metadata  
- global variables  

Memory loads and updates automatically during routing.

---

# ğŸŒ API Endpoints

```
POST /api/chat
POST /api/generate
GET  /api/memory
```

You can use the bridge as a standalone AI backend.

---

# ğŸ§© Creating Custom Adapters

Adapters consist of:

- input parsers  
- output formatters  

You can connect:

- web interfaces  
- Discord bots  
- custom dashboards  
- CLI tools  
- AnythingLLM  
- LobeChat  

---

# ğŸš§ Roadmap

- [ ] Automatic agent routing  
- [ ] Optional vector memory  
- [ ] Web dashboard  
- [ ] Plugin system  
- [ ] Unit tests  
- [ ] API authentication  
- [ ] Live JSON log viewer  

---

# ğŸ“„ License

Licensed under **CC BY-NC 4.0**.  
Commercial use is not permitted.

---

# â¤ï¸ Maintainer

Developed by **Danny**.  
Issues and contributions are welcome.
