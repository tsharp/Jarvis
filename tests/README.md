# tests/README.md
# ðŸ§ª Test Suite fÃ¼r Jarvis

## Quick Start

```bash
# Dependencies installieren
pip install -r requirements-dev.txt

# Alle Tests ausfÃ¼hren
pytest

# Mit Coverage
pytest --cov=. --cov-report=html

# Nur bestimmte Tests
pytest tests/test_json_parser.py
pytest -k "test_valid"  # Nur Tests mit "test_valid" im Namen
```

## Test-Struktur

```
tests/
â”œâ”€â”€ conftest.py          # Fixtures (wiederverwendbare Test-Daten)
â”œâ”€â”€ test_json_parser.py  # JSON-Parser Tests (KRITISCH)
â”œâ”€â”€ test_models.py       # Datenmodell Tests
â”œâ”€â”€ test_api.py          # API-Endpoint Tests
â””â”€â”€ test_persona.py      # Persona-System Tests
```

## Was wird getestet?

### ðŸ”´ Kritisch (test_json_parser.py)
- Valides JSON parsing
- JSON in Markdown-Codeblocks
- Trailing commas
- Python True/False/None â†’ JSON
- LLM-typische Probleme

### ðŸŸ¡ Wichtig (test_models.py)
- Message Konvertierungen
- Request/Response Erstellung
- get_last_user_message()

### ðŸŸ¢ Nice-to-have (test_api.py)
- Endpoint-Existenz
- CORS-Konfiguration
- Error Handling

## Lokale Tests vs CI

```bash
# Lokal (ohne Ollama/MCP)
pytest tests/test_json_parser.py tests/test_models.py

# Mit laufenden Services
pytest  # Alle Tests
```

## Neue Tests hinzufÃ¼gen

1. Erstelle `tests/test_<modul>.py`
2. Benutze Fixtures aus `conftest.py`
3. Prefix: `test_` fÃ¼r Funktionen
4. Beschreibende Namen!

```python
def test_what_it_does_when_given_this():
    """Docstring erklÃ¤rt was getestet wird."""
    result = function_under_test(input)
    assert result == expected
```

## Coverage anzeigen

```bash
pytest --cov=. --cov-report=term-missing
```
