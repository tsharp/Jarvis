# docker-compose.yml (Mit Sequential Thinking + CIM MCP Services)
#
# Services:
# - lobechat-adapter:     Port 8100 → LobeChat trägt diese URL ein
# - jarvis-admin-api:     Port 8200 → Admin API + Chat für JarvisWebUI
# - jarvis-webui:         Port 8400 → Frontend
# - mcp-sql-memory:       Port 8082 → Memory MCP Server
# - cim-server:           Port 8086 → Frank's CIM MCP Server (NEU!)
# - sequential-thinking:  Port 8085 → Sequential Thinking MCP Server
# - validator-service:    Port 8300 → Validator
#
# Netzwerk: big-bear-lobe-chat_default (extern, muss existieren)
#
# SHARED VOLUMES: Core-Code wird live gemountet (kein Rebuild nötig!)

services:
  # ============================================================
  # LOBECHAT ADAPTER
  # ============================================================
  lobechat-adapter:
    build:
      context: .
      dockerfile: adapters/lobechat/Dockerfile
    container_name: lobechat-adapter
    ports:
      - "8100:8100"
    environment:
      # Service URLs
      - OLLAMA_BASE=http://ollama:11434
      - MCP_BASE=http://mcp-sql-memory:8081/mcp
      - VALIDATOR_URL=http://validator-service:8000
      
      # Model Konfiguration (änderbar!)
      - THINKING_MODEL=deepseek-r1:8b
      - CONTROL_MODEL=qwen3:4b
      - EMBEDDING_MODEL=hellord/mxbai-embed-large-v1:f16
      
      # Layer Toggles (Speed-Optimierung!)
      - ENABLE_CONTROL_LAYER=true
      - SKIP_CONTROL_ON_LOW_RISK=true
      
      # Validation
      - ENABLE_VALIDATION=true
      - VALIDATION_HARD_FAIL=true
      - LOG_LEVEL=INFO
    volumes:
      - ./config:/app/config:ro
      - ./personas:/app/personas
      # SHARED VOLUMES - Core Code live gemountet!
      - ./core:/app/core:ro
      - ./intelligence_modules:/app/intelligence_modules:ro
      - ./mcp_registry.py:/app/mcp_registry.py:ro
      - ./config.py:/app/config.py:ro
      - ./mcp:/app/mcp:ro
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
      - mcp-sql-memory
      - cim-server
      - sequential-thinking

  # ============================================================
  # JARVIS WEB UI (Debug/Chat Interface)
  # ============================================================
  jarvis-webui:
    build:
      context: adapters/Jarvis
      dockerfile: Dockerfile
    container_name: jarvis-webui
    ports:
      - "8400:80"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
      - jarvis-admin-api

  # ============================================================
  # JARVIS ADMIN API (Management API for WebUI)
  # ============================================================
  jarvis-admin-api:
    build:
      context: .
      dockerfile: adapters/admin-api/Dockerfile
    container_name: jarvis-admin-api
    ports:
      - "8200:8200"
    environment:
      - MCP_BASE=http://mcp-sql-memory:8081/mcp
      - CIM_URL=http://cim-server:8086
      - SEQUENTIAL_THINKING_URL=http://sequential-thinking:8085
      - LOG_LEVEL=INFO
    volumes:
      - ./personas:/app/personas
      # SHARED VOLUMES - Core Code live gemountet!
      - ./core:/app/core:ro
      - ./intelligence_modules:/app/intelligence_modules:ro
      - ./mcp_registry.py:/app/mcp_registry.py:ro
      - ./config.py:/app/config.py:ro
      - ./mcp:/app/mcp:ro
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
      - mcp-sql-memory
      - cim-server
      - sequential-thinking

  # ============================================================
  # MCP SQL MEMORY SERVER
  # ============================================================
  mcp-sql-memory:
    build:
      context: ./sql-memory
    container_name: mcp-sql-memory
    environment:
      - DB_PATH=/app/data/memory.db
      - OLLAMA_URL=http://ollama:11434
      - EMBEDDING_MODEL=hellord/mxbai-embed-large-v1:f16
    volumes:
      - ./sql-memory/data:/app/data
    ports:
      - "8082:8081"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped

  # ============================================================
  # CIM SERVER - Frank's Causal Intelligence Module (NEU!)
  # ============================================================
  cim-server:
    build:
      context: ./mcp-servers/cim-server
    container_name: cim-server
    environment:
      - CIM_ROOT=/app/intelligence_modules
      - OLLAMA_BASE=http://ollama:11434
      - LOG_LEVEL=INFO
    volumes:
      # CIM hat exklusiven Zugriff auf intelligence_modules
      - ./intelligence_modules:/app/intelligence_modules
    ports:
      - "8086:8086"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped

  # ============================================================
  # SEQUENTIAL THINKING MCP SERVER
  # ============================================================
  sequential-thinking:
    build:
      context: ./mcp-servers/sequential-thinking
    container_name: sequential-thinking
    environment:
      - MCP_SERVER_MODE=true
      - CIM_URL=http://cim-server:8086
      - OLLAMA_BASE=http://ollama:11434
      - LOG_LEVEL=INFO
    ports:
      - "8085:8085"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped
    depends_on:
      - cim-server

  # ============================================================
  # VALIDATOR SERVICE
  # ============================================================
  validator-service:
    build:
      context: ./validator-service/validator-service
    container_name: validator-service
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - EMBEDDING_MODEL=hellord/mxbai-embed-large-v1:f16
      - VALIDATOR_MODEL=qwen2.5:0.5b-instruct
    ports:
      - "8300:8000"
    networks:
      - big-bear-lobe-chat_default
    restart: unless-stopped

networks:
  big-bear-lobe-chat_default:
    external: true
